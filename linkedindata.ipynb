{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "460ec337-5b4a-4f8e-8643-4f977e509efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "['D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-070.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-071.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-072.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-073.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-074.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-075.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-076.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-077.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-078.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-079.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-080.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-081.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-082.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-083.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-084.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-085.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-086.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-087.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-088.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-089.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-090.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-091.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-092.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-093.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-094.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-095.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-096.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-097.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-098.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-099.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-100.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-101.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-102.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-103.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-104.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-105.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-106.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-107.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-108.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-109.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-110.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-111.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-112.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-113.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-114.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-115.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-116.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-117.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-118.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-119.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-120.csv', 'D:/DataInc/PDL-1-2B-Splitted\\\\1.2 Billion PeopleDataLabs Database-121.csv']\n",
      "Number of matched files: 52\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from  pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from datetime import datetime\n",
    "import time \n",
    "from pyspark.sql.functions import col, split, when, size,count,row_number,desc,trim,regexp_replace\n",
    "\n",
    "# Define the window specification\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'\n",
    "print(\"completed\")\n",
    "\n",
    "file_pattern = r\"D:/DataInc/PDL-1-2B-Splitted/1.2 Billion PeopleDataLabs Database-07*.csv\"\n",
    "file_pattern1 =r\"D:/DataInc/PDL-1-2B-Splitted/1.2 Billion PeopleDataLabs Database-08*.csv\"\n",
    "file_pattern2 =r\"D:/DataInc/PDL-1-2B-Splitted/1.2 Billion PeopleDataLabs Database-09*.csv\"\n",
    "file_pattern3 =r\"D:/DataInc/PDL-1-2B-Splitted/1.2 Billion PeopleDataLabs Database-1*.csv\"\n",
    "files = glob.glob(file_pattern)\n",
    "files1 = glob.glob(file_pattern1)\n",
    "files2 = glob.glob(file_pattern2)\n",
    "files3 = glob.glob(file_pattern3)\n",
    "#print(f\"Number of matched files: {len(files)}\")\n",
    "file=files+files1+files2+files3\n",
    "print(file)\n",
    "print(f\"Number of matched files: {len(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69e2ddb5-4cdb-414e-8c8b-7a7452735cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark started reading the file\n",
      "Process ended at: 2024-12-01 01:34:44.488909\n",
      "Total duration: 0:00:01.248443\n",
      "PySpark job ran successfully on the local machine.\n",
      "[('spark.app.name', 'LocalPySparkTest'), ('spark.sql.shuffle.partitions', '480'), ('spark.app.startTime', '1733041680668'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1733041682552'), ('spark.sql.warehouse.dir', 'file:/C:/spark/spark-3.1.2-bin-hadoop3.2/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.driver.extraClassPath', 'C:/Users/Administrator/Downloads/postgresql-42.7.4.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.port', '55505'), ('spark.driver.host', 'HV01'), ('spark.driver.memory', '200g'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Initialize SparkSession for local mode\n",
    "    spark = SparkSession.builder.appName(\"LocalPySparkTest\").config(\"spark.driver.extraClassPath\", \"C:/Users/Administrator/Downloads/postgresql-42.7.4.jar\")\\\n",
    "    .config(\"spark.driver.memory\", \"200g\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"480\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "    #\n",
    "    \n",
    "\n",
    "    shema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),  # Replace with your column names and data types\n",
    "    StructField(\"Email_List\", StringType(), True),\n",
    "    StructField(\"Phone_List\", StringType(), True),\n",
    "    StructField(\"LinkedInURL\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True)\n",
    "    ])\n",
    "    # Create a DataFrame\n",
    "    #rdd = spark.sparkContext.parallelize(data)\n",
    "    #file_path = r\"C:/Users/Administrator/Downloads/tasks.csv\"\n",
    "    #file_path=r\"D:/DataInc/PDL-1-2B-Splitted/1.2 Billion PeopleDataLabs Database-000.csv\"\n",
    "    file_path=r\"D:/DataInc/PDL-1-2B-Splitted/1.2 Billion PeopleDataLabs Database-12*.csv\"\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    #df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    print(\"Spark started reading the file\")\n",
    "    start_time = datetime.now()\n",
    "    dfs=[spark.read.format(\"csv\").option(\"header\", \"false\").option(\"inferSchema\", \"false\").option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\\\") \\\n",
    "    .option(\"lineSep\", \"\\n\")\\\n",
    "    .schema(shema)\\\n",
    "    .load(f) for f in file]\n",
    "    # Print success message\n",
    "    df = dfs[0]\n",
    "    for other_df in dfs[1:]:\n",
    "        df = df.union(other_df)\n",
    "    \n",
    "    # Record the end time\n",
    "    end_time = datetime.now()\n",
    "    print(f\"Process ended at: {end_time}\")\n",
    "    \n",
    "    # Calculate the duration\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Total duration: {duration}\")\n",
    "    print(\"PySpark job ran successfully on the local machine.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Print error message if job fails\n",
    "    print(f\"PySpark job failed with error: {e}\")\n",
    "\n",
    "print(spark.sparkContext._conf.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a85fc3f-e836-4923-b6a5-bdb741ddba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted duplicate and null records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df=df.filter(((col('Email_List').isNotNull()) &  (col('Email_List') != \"\") & (col('Email_List') != \"null\")) | ((col('Phone_List').isNotNull()) &  (col('Phone_List') != \"\") &  (col('Phone_List') != \"null\")))\n",
    "windowSpec = Window.partitionBy(\"LinkedInURL\").orderBy(col(\"Email_List\").asc())\n",
    "df = df.withColumn(\"row_number\", row_number().over(windowSpec)).filter(col('row_number')==1).drop(\"row_number\").filter(col(\"LinkedInURL\").isNotNull())\n",
    "print(\"deleted duplicate and null records\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a38b7ae0-8a95-4343-b8e1-91a0de58b78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores available: 24\n"
     ]
    }
   ],
   "source": [
    "#df.select(*df.columns[:4]).count()\n",
    "#df.count()#select(['_c0','_c4']).show(4,truncate=False)\n",
    "\n",
    "#columns=[\"name\",\"email_list\",\"phone_list\",\"URL\",\"address\"]\n",
    "#df.toDF(*columns)\n",
    "#df.rdd.getNumPartitions()#show(4,truncate=False)\n",
    "#df1=df.groupBy(col(\"URL\")).agg(count('*').alias('count_d'))\n",
    "#df1.filter(col('count_d')>1).show(truncate=False)\n",
    "#df.count()\n",
    "#print(spark.sparkContext.getConf().get(\"spark.driver.cores\"))\n",
    "#print(spark.sparkContext.getConf().get(\"spark.executor.cores\"))\n",
    "num_cores = spark.sparkContext.defaultParallelism\n",
    "\n",
    "print(f\"Number of cores available: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6540b79a-a581-4e1d-81c9-f38984eca475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails and phone numbers and names are transformed\n"
     ]
    }
   ],
   "source": [
    "#first_name last _name deriving Logic\n",
    "#df.select(\"name\").filter(size(split(col(\"name\"),\" \"))>4).\n",
    "#split the name into fname and lname\n",
    "\n",
    "#df_name=df.withColumn(\"Fname\",split(col(\"name\"),\" \").getItem(0)).withColumn(\"Lname\",split(col(\"name\"),\" \",2).getItem(1))\n",
    "# Split the name column into Fname and Lname, and trim the results\n",
    "df_name = df.withColumn(\"Fname\", trim(split(col(\"Name\"), \" \").getItem(0))) \\\n",
    "            .withColumn(\"Lname\", trim(split(col(\"Name\"), \" \", 2).getItem(1)))\n",
    "\n",
    "# Show the result\n",
    "#df_name.select(\"name\", \"Fname\", \"Lname\").show(40,truncate=False)\n",
    "#df.withColumn(\"split_email\",split(col(\"Email_list\"),\",\")).select(['Email_list','split_email']).show(200,truncate=False)\n",
    "df_email_split = df_name.withColumn(\"email_array\", split(col(\"Email_List\"), \",\"))\n",
    "\n",
    "# Derive WEmail (index 0) and PEmail (index 1)\n",
    "df_emails = df_email_split \\\n",
    "    .withColumn(\"WEmail\", when(size(col(\"email_array\")) > 0, trim(col(\"email_array\")[0])).otherwise(None)) \\\n",
    "    .withColumn(\"PEmail\", when(size(col(\"email_array\")) > 1, trim(col(\"email_array\")[1])).otherwise(None)).drop(\"email_array\")\n",
    "#df_emails.select([\"email_list\",\"WEmail\",\"PEmail\"]).show(200,truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "df_split = df_emails.withColumn(\"phone_array\", split(col(\"Phone_List\"), \",\"))\n",
    "\n",
    "# Derive WPhone, HPhone, and MPhone with trimming\n",
    "df_phones = df_split \\\n",
    "    .withColumn(\"WPhone\", when(size(col(\"phone_array\")) > 0, trim(col(\"phone_array\")[0])).otherwise(None)) \\\n",
    "    .withColumn(\"HPhone\", when(size(col(\"phone_array\")) > 1, trim(col(\"phone_array\")[1])).otherwise(None)) \\\n",
    "    .withColumn(\"MPhone\", when(size(col(\"phone_array\")) > 2, trim(col(\"phone_array\")[2])).otherwise(None)).drop(\"phone_array\")\n",
    "\n",
    "\n",
    "# Show the result\n",
    "#df_phones.select(\"phone_list\", \"WPhone\", \"HPhone\", \"MPhone\").show(200,truncate=False)\n",
    "print(\"emails and phone numbers and names are transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5e23679-3e50-4219-bec0-0a3f3ba9ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address transformation completed\n"
     ]
    }
   ],
   "source": [
    "df = df_phones.withColumn(\"split_location\", split(col(\"Address\"), \",\")) \n",
    "#df.select(['address','split_location']).where(size(col(\"split_location\"))>3).show(100,truncate=False)\n",
    "df = df.withColumn(\"Country\",when(size(col(\"split_location\")) == 4, trim(col(\"split_location\")[3]))\n",
    "    .when(size(col(\"split_location\")) == 3, trim(col(\"split_location\")[2]))\n",
    "    .when(size(col(\"split_location\")) == 2, trim(col(\"split_location\")[1]))\n",
    "    .when(size(col(\"split_location\")) == 1, trim(col(\"split_location\")[0]))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"State\",when(size(col(\"split_location\")) == 4, trim(col(\"split_location\")[1]))\n",
    "    .when(size(col(\"split_location\")) == 3, trim(col(\"split_location\")[1]))\n",
    "    .when(size(col(\"split_location\")) == 2, trim(col(\"split_location\")[1]))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"City\",when(size(col(\"split_location\")) == 4 , trim(col(\"split_location\")[0]))\n",
    "    .when(size(col(\"split_location\")) == 3 , trim(col(\"split_location\")[0]))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "df_final=df.drop(\"split_location\")#.where(size(col(\"split_location\"))>3).show(100,truncate=False)\n",
    "print(\"address transformation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d46d596-1120-48f4-8a88-4ebe121bbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean up start\n",
      "clean up done\n"
     ]
    }
   ],
   "source": [
    "print(\"clean up start\")\n",
    "for column in df_final.columns:\n",
    "    df_final = df_final.withColumn(column, regexp_replace(col(column), \"\\\\x00\", \"\"))\n",
    "print(\"clean up done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12d2f73c-c82a-4381-916d-c881cc6c7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at: 2024-12-01 01:34:45.362738\n",
      "Process ended at: 2024-12-01 02:15:28.935766\n",
      "Total duration: 0:40:43.573028\n",
      "data has been written to master table in postgress\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Load data from PostgreSQL table\\ndf = spark.read     .format(\"jdbc\")     .option(\"url\", jdbc_url)     .option(\"dbtable\", \"test\").options(**connection_properties)     .load()\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"  # Replace with your details\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",         # Replace with your username\n",
    "    \"password\": \"*****\", # Replace with your password\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(f\"Process started at: {start_time}\")\n",
    "\n",
    "\n",
    "df_final.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"pdl_data\").options(**connection_properties) \\\n",
    "    .mode(\"append\").save()  # Use 'overwrite', 'append', or 'ignore' based on your use case\n",
    "\n",
    "# Record the end time\n",
    "end_time = datetime.now()\n",
    "print(f\"Process ended at: {end_time}\")\n",
    "\n",
    "# Calculate the duration\n",
    "duration = end_time - start_time\n",
    "print(f\"Total duration: {duration}\")\n",
    "print(\"data has been written to master table in postgress\")\n",
    "\n",
    "\"\"\"\n",
    "# Load data from PostgreSQL table\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"test\").options(**connection_properties) \\\n",
    "    .load()\n",
    "\"\"\"\n",
    "# Show data\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "098a0051-88f7-4ef5-bbaa-5c1065dd5510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178740352"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
