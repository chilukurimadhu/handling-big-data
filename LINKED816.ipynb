{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b725f98-f71c-4f88-bd71-93265684c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from  pyspark.sql.types import StructType, StructField, StringType, IntegerType,LongType\n",
    "from datetime import datetime\n",
    "import time \n",
    "from pyspark.sql.functions import col, split, when, size,count,row_number,desc,trim,regexp_replace,lit\n",
    "import gc\n",
    "# Define the window specification\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'\n",
    "print(\"completed\")\n",
    "\n",
    "\n",
    "\n",
    "file_pattern = r\"D:/DataInc-New/Linkedin816MSplittedCSV/linkedin_db-00*.csv\"\n",
    "#file_pattern = r\"D:/DataInc/Linkedin816m-Splitted/LinkedInComplete.csv_split_*.csv\"\n",
    "print(file_pattern)\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "print(f\"Number of matched files: {len(files)}\")\n",
    "\n",
    "print(files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a9519-d767-4626-9e43-3c05ec93d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files(files):\n",
    "    try:\n",
    "        # Initialize SparkSession for local mode\n",
    "        spark = SparkSession.builder.appName(\"LocalPySparkTest\").config(\"spark.driver.extraClassPath\", \"C:/Users/****/Downloads/postgresql-42.7.4.jar\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"480\") \\\n",
    "        .config(\"spark.driver.memory\", \"200g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"10g\")\\\n",
    "        .master(\"local[*]\").getOrCreate()  #.config(\"spark.driver.memory\", \"63g\")\\.config(\"spark.driver.maxResultSize\", \"60g\") \\\n",
    "        \n",
    "        # Define schema\n",
    "        shema = StructType([\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"Email\", StringType(), True),\n",
    "        StructField(\"PhoneNumber\", StringType(), True),\n",
    "        StructField(\"LinkedIn\", StringType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Company Name\", StringType(), True),\n",
    "        StructField(\"CompanyPhone\", StringType(), True),\n",
    "        StructField(\"Website\", StringType(), True),\n",
    "        StructField(\"Website2\", StringType(), True),\n",
    "        StructField(\"Facebook\", StringType(), True),\n",
    "        StructField(\"Twitter\", StringType(), True),\n",
    "        StructField(\"LinkedinCompanyPage\", StringType(), True),\n",
    "        StructField(\"Country\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "\n",
    "        columns = [\n",
    "        \"first_name\", \"last_name\", \"gender\", \"noOfEmails\", \"emails\", \"phone_numbers\",\n",
    "        \"job_title\", \"job_title_role\", \"job_title_sub_role\", \"job_title_levels\",\n",
    "        \"job_company_name\", \"job_company_website\", \"job_company_size\", \"job_company_founded\",\n",
    "        \"job_company_industry\", \"inferred_salary\", \"inferred_years_experience\", \"industry\",\n",
    "        \"skills\", \"interests\", \"job_company_location_name\", \"job_company_location_locality\",\n",
    "        \"job_company_location_metro\", \"job_company_location_region\", \"job_company_location_geo\",\n",
    "        \"job_company_location_street_address\", \"job_company_location_postal_code\",\n",
    "        \"job_company_location_country\", \"job_company_location_continent\", \"location_name\",\n",
    "        \"location_locality\", \"location_metro\", \"location_region\", \"location_country\",\n",
    "        \"location_continent\", \"location_street_address\", \"location_postal_code\", \"location_geo\",\n",
    "        \"location_names\", \"street_addresses\", \"regions\", \"countries\", \"experience_companies_names\",\n",
    "        \"experience_companies_industries\", \"experience_companies_job_titles\",\n",
    "        \"experience_companies_job_roles\", \"experience_companies_job_sub_roles\",\n",
    "        \"experience_companies_job_levels\", \"linkedin_url\", \"facebook_url\", \"twitter_url\",\n",
    "        \"job_company_linkedin_url\", \"job_company_facebook_url\", \"job_company_twitter_url\",\n",
    "        \"education_degrees\", \"education_majors\", \"profiles\", \"linkedin_connections\",\n",
    "        \"certifications\", \"languages\",\"unknown\"\n",
    "        ]\n",
    "    \n",
    "        # Read the CSV file into a DataFrame\n",
    "        #df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        print(\"Spark started reading the file\")\n",
    "        start_time = datetime.now()\n",
    "        dfs=[spark.read.format(\"csv\").option(\"header\", \"false\").option(\"inferSchema\", \"false\").option(\"delimiter\", \",\") \\\n",
    "        .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\\\") \\\n",
    "        .option(\"lineSep\", \"\\n\")\\\n",
    "        #.schema(shema)\\\n",
    "        .load(f) for f in files]\n",
    "        # Print success message\n",
    "        df = dfs[0]\n",
    "        for other_df in dfs[1:]:\n",
    "            df = df.union(other_df)\n",
    "        df = df.toDF(*columns)\n",
    "        \n",
    "        # Record the end time\n",
    "        end_time = datetime.now()\n",
    "        print(f\"Process ended at: {end_time}\")\n",
    "        \n",
    "        # Calculate the duration\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Total duration: {duration}\")\n",
    "        print(\"PySpark job ran successfully on the local machine.\")\n",
    "        return df,spark\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Print error message if job fails\n",
    "        print(f\"PySpark job failed with error: {e}\")\n",
    "\n",
    "print('read cell execution success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d310c06c-5dda-4219-8e22-308e3559f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dups(df):\n",
    "    \n",
    "    windowSpec = Window.partitionBy(\"linkedin_url\").orderBy(col(\"emails\").asc(),col(\"phone_numbers\").asc())\n",
    "    df1 = df.withColumn(\"row_number\", row_number().over(windowSpec)).filter(col('row_number')==1).drop(\"row_number\")#.filter(col(\"LinkedInURL\").isNotNull())\n",
    "    print(\"deleted duplicate and null records\")\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7d107-8236-4b7c-aeb8-a4f29abfb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_and_split(df1):\n",
    "    #windowSpec = Window.partitionBy(\"LinkedIn\").orderBy(col(\"id\").asc())\n",
    "    #df1 = df.withColumn(\"row_number\", row_number().over(windowSpec)).filter(col('row_number')==1).drop(\"row_number\")#.filter(col(\"LinkedInURL\").isNotNull())\n",
    "    #print(\"deleted duplicate and null records\")\n",
    "    #df1.select(['LinkedIn']).show(truncate=False)\n",
    "    # Trim only string columns\n",
    "    for column, dtype in df1.dtypes:\n",
    "        if dtype == \"string\":\n",
    "            df1 = df1.withColumn(column, trim(col(column)))\n",
    "    print(\"data trimmed successfully\")\n",
    "    \n",
    "    #df_name_split=df1.withColumn(\"Fname\", trim(split(col(\"Name\"), \" \").getItem(0))) \\\n",
    "    #           .withColumn(\"Lname\", trim(split(col(\"Name\"), \" \", 2).getItem(1)))\n",
    "    \n",
    "    \n",
    "    #df_final=df_name_split.select('*')\n",
    "    \"\"\"(['id',\n",
    "     'Name','Fname',\n",
    "     'Lname',\n",
    "     'Email',\n",
    "     'PhoneNumber',\n",
    "     'LinkedIn',\n",
    "     'Title',\n",
    "     'Company Name',\n",
    "     'CompanyPhone',\n",
    "     'Website',\n",
    "     'Website2',\n",
    "     'Facebook',\n",
    "     'Twitter',\n",
    "     'LinkedinCompanyPage',\n",
    "     'Country',\n",
    "      'batch_count'                            \n",
    "     ])\"\"\"\n",
    "    return df1\n",
    "\n",
    "print('trim cell execution success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1feb8b58-d65d-4213-918b-b1c767d32df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_colum(df_name):\n",
    "    df_email_split = df_name.withColumn(\"email_array\", split(col(\"emails\"), \",\"))\n",
    "    \n",
    "    # Derive WEmail (index 0) and PEmail (index 1)\n",
    "    df_emails = df_email_split \\\n",
    "        .withColumn(\"WEmail\", when(size(col(\"email_array\")) > 0, trim(col(\"email_array\")[0])).otherwise(None)) \\\n",
    "        .withColumn(\"PEmail\", when(size(col(\"email_array\")) > 1, trim(col(\"email_array\")[1])).otherwise(None)).drop(\"email_array\")\n",
    "    #df_emails.select([\"email_list\",\"WEmail\",\"PEmail\"]).show(200,truncate=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_split = df_emails.withColumn(\"phone_array\", split(col(\"phone_numbers\"), \",\"))\n",
    "    \n",
    "    # Derive WPhone, HPhone, and MPhone with trimming\n",
    "    df_phones = df_split \\\n",
    "        .withColumn(\"WPhone\", when(size(col(\"phone_array\")) > 0, trim(col(\"phone_array\")[0])).otherwise(None)) \\\n",
    "        .withColumn(\"HPhone\", when(size(col(\"phone_array\")) > 1, trim(col(\"phone_array\")[1])).otherwise(None)) \\\n",
    "        .withColumn(\"MPhone\", when(size(col(\"phone_array\")) > 2, trim(col(\"phone_array\")[2])).otherwise(None)).drop(\"phone_array\")\n",
    "    \n",
    "    \n",
    "    # Show the result\n",
    "    #df_phones.select(\"phone_list\", \"WPhone\", \"HPhone\", \"MPhone\").show(200,truncate=False)\n",
    "    print(\"emails and phone numbers and names are transformed\")\n",
    "    return df_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce13b214-2307-4eb0-ad9f-443d043f6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_postgres(df_final,batch):\n",
    "\n",
    "    print(\"clean up start\")\n",
    "    for column in df_final.columns:\n",
    "        df_final = df_final.withColumn(\n",
    "            column,\n",
    "            when((col(column).rlike(\"\\\\x00\") |  (col(column) == \"\")), None).otherwise(col(column))\n",
    "        )\n",
    "\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"  # Replace with your details\n",
    "    connection_properties = {\n",
    "        \"user\": \"postgres\",         # Replace with your username\n",
    "        \"password\": \"*******\", # Replace with your password\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(f\"Process started at: {start_time}\")\n",
    "    \n",
    "    df_final=df_final.withColumn('batch_count',lit(batch))\n",
    "    df_final.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"LINKEDIN816_DATA\").options(**connection_properties) \\\n",
    "        .mode(\"append\").save()  # Use 'overwrite', 'append', or 'ignore' based on your use case\n",
    "    \n",
    "    # Record the end time\n",
    "    end_time = datetime.now()\n",
    "    print(f\"Process ended at: {end_time}\")\n",
    "    \n",
    "    # Calculate the duration\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Total duration: {duration}\")\n",
    "    print(\"data has been written to master table in postgress\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bbe4c0-eece-4a1f-89f8-f1f4fc03b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the files\n",
    "folder_path = r\"D:/DataInc-New/Linkedin816MSplittedCSV\"\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = sorted([os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".csv\")])\n",
    "\n",
    "# Function to process files in batches\n",
    "def process_files_in_batches(files, batch_size):\n",
    "    total_files = len(files)\n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1}: {len(batch_files)} files\")\n",
    "        batch_count=(i // batch_size + 1)\n",
    "        #print(batch_files)\n",
    "        #if batch_count>18:\n",
    "        df_read,spark=read_files(batch_files)\n",
    "        #df_read.show()\n",
    "        print('files read df successfull')\n",
    "        \n",
    "        der_df=deriv_colum(df_read)\n",
    "        #df_trimmed.show()\n",
    "        #dup_df=find_dups(df_read)\n",
    "        #dup_df.show()\n",
    "        df_trimmed=trimmed_and_split(der_df)\n",
    "        print('trim successfull')\n",
    "        wr_df=write_postgres(df_trimmed,batch_count)\n",
    "        print('write df successfull')\n",
    "        df_read.unpersist(blocking=True) if df_read.is_cached else None\n",
    "        df_trimmed.unpersist(blocking=True) if df_trimmed.is_cached else None\n",
    "        der_df.unpersist(blocking=True) if df_read.is_cached else None\n",
    "        wr_df.unpersist(blocking=True) if df_read.is_cached else None\n",
    "        print(\"unpersisted process\")\n",
    "        #return wr_df\n",
    "        # Trigger garbage collection\n",
    "        gc.collect()\n",
    "        print(\"garbage  process\")\n",
    "        # Clear temporary cache\n",
    "        spark.catalog.clearCache()\n",
    "        print(\"catalog   process\")\n",
    "        spark.stop()\n",
    "        #else:\n",
    "        #print(f\"batch count is skipped {batch_count}\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "process_files_in_batches(all_files,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eab953-6b12-490a-ba9d-7cf30266e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_configs = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "# Print all configurations\n",
    "for key, value in all_configs:\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
